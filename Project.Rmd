#Training

##Cleaning

We start by loading the training dataset. Looking at the file shows several different problems: some values are empty, some are marked as NA and some are *#/DIV/0*. We will consider all of the cases as NAs.

```{r}
setwd("~/Documents/Coursera/8 - Machine learning")
training <- read.csv("pml-training.csv", na.strings=c("NA","","#DIV/0"))

head(training, n=3)
```

We can see that the first columns are metadata (timestamps, ...). We don't want to use them in our model to avoid overfitting.
However I decide to keep the user names since they are the same ones as in the test dataset and could thus provide some useful information.

```{r}
training <- training[,-c(1,3,4,5,6,7)]
head(training, n=3)
```

Some variables seem to have a lot of NAs.  Let's look as the NA rate for every variable.

```{r}
na_rate <- function(x)
{
  sum(is.na(x))/length(x)    
}
    
sapply(training, na_rate)
```

A lot of these variables are unavailable in 98% of the cases. We should remove them as this could lead to some overfitting in the remaining 2% cases.

```{r}
dim(training)
training <- training[,sapply(training, na_rate) < 0.9]
dim(training)
```

##Modeling


For cross-validation, we split our training data between a training set and a test set. Unfortunately, the model we will use takes a lot of time on my poor CPU, I will have to keep the training set relatively small.

```{r}
set.seed(1)
library(caret)
in_train <- createDataPartition(training$classe, p=0.25, list = FALSE)
training_train <- training[in_train,]
training_test <- training[-in_train,]
```

We choose to use the Random Forest model. It's a simple and very efficient model, which doesn't need any hypothesis and thus decrease the risks of overfitting.

```{r, cache=TRUE}
model <- train(classe~.,method="rf", data=training_train)
confusionMatrix(predict(model,newdata=training_train),training_train$classe)
```

That's a perfect match! The in-sample error is zero!
Let's apply it to the test set.

```{r}
confusionMatrix(predict(model,newdata=training_test),training_test$classe)
```

98% accuracy! It's a very good result for such a simple analysis and it could maybe even be improved by increasing the size of the training dataset.

#Submission

We apply the same model to the testing dataset.

```{r}
testing <- read.csv("pml-testing.csv", na.strings=c("NA","","#DIV/0"))
testing <- testing[,-c(1,3,4,5,6,7)]
answers <- predict(model, newdata = testing)
answers
```

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
  }

pml_write_files(answers)
```
